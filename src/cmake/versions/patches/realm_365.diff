diff --git a/src/realm/cuda/cuda_internal.cc b/src/realm/cuda/cuda_internal.cc
index 0df4e33dd8..ff5be824e0 100644
--- a/src/realm/cuda/cuda_internal.cc
+++ b/src/realm/cuda/cuda_internal.cc
@@ -2223,6 +2223,10 @@ namespace Realm {
       GPU *gpu = checked_cast<GPUreduceChannel *>(channel)->gpu;
       stream = gpu->get_next_d2d_stream();

+      // Per-GPU kernel caching: Check if we already have a kernel for this specific GPU
+      // IMPORTANT: CUfunction pointers are context-specific - a kernel obtained for
+      // GPU0's context cannot be used on GPU1's context. In multi-GPU setups, each GPU
+      // must have its own cached kernel obtained in the correct CUDA context.
       {
         AutoLock<Mutex> al(gpu->alloc_mutex);
         std::unordered_map<ReductionOpID, GPU::GPUReductionOpEntry>::const_iterator
@@ -2244,15 +2248,41 @@ namespace Realm {
                                             : redop->cuda_fold_nonexcl_fn)
                  : (redop_info.is_exclusive ? redop->cuda_apply_excl_fn
                                             : redop->cuda_apply_nonexcl_fn));
+
         if(redop->cudaGetFuncBySymbol_fn != 0) {
-          // we can ask the runtime to perform the mapping for us
+          // We can ask the runtime to perform the mapping for us
+          // CRITICAL: Must obtain the kernel within the correct GPU's CUDA context
+          // to ensure the CUfunction pointer is valid for this specific GPU
           gpu->push_context();
+
 #ifdef REALM_USE_CUDART_HIJACK
           ThreadLocal::current_gpu_stream = stream;
 #endif
-          CHECK_CUDART(reinterpret_cast<PFN_cudaGetFuncBySymbol>(
-              redop->cudaGetFuncBySymbol_fn)((void **)&kernel, host_proxy));
+
+          int result = reinterpret_cast<PFN_cudaGetFuncBySymbol>(
+              redop->cudaGetFuncBySymbol_fn)((void **)&kernel, host_proxy);
+          CHECK_CUDART(result);
+
           gpu->pop_context();
+
+          // Cache this kernel in the GPU's local reduction table for future reuse
+          // This avoids repeated cudaGetFuncBySymbol calls and ensures each GPU
+          // has its own context-specific kernel instance
+          {
+            AutoLock<Mutex> al(gpu->alloc_mutex);
+            GPU::GPUReductionOpEntry &entry = gpu->gpu_reduction_table[redop_info.id];
+            if(redop_info.is_fold) {
+              if(redop_info.is_exclusive)
+                entry.fold_excl = kernel;
+              else
+                entry.fold_nonexcl = kernel;
+            } else {
+              if(redop_info.is_exclusive)
+                entry.apply_excl = kernel;
+              else
+                entry.apply_nonexcl = kernel;
+            }
+          }
         } else {
           // no way to ask the runtime to perform the mapping, so we'll have
           //  to actually launch the kernels with the runtime API using the launch
@@ -2418,20 +2448,27 @@ namespace Realm {
               args->count = elems;

               size_t threads_per_block = 256;
-              size_t blocks_per_grid = 1 + ((elems - 1) / threads_per_block);
+              size_t blocks_per_grid = std::min(1 + ((elems - 1) / threads_per_block),
+                                                static_cast<size_t>(CUDA_MAX_BLOCKS_PER_GRID));

               {
                 AutoGPUContext agc(channel->gpu);

                 if(kernel != 0) {
-                  void *extra[] = {CU_LAUNCH_PARAM_BUFFER_POINTER, args,
-                                   CU_LAUNCH_PARAM_BUFFER_SIZE, &args_size,
-                                   CU_LAUNCH_PARAM_END};
-
+                  // Use params array to pass kernel arguments (pointers to each parameter)
+                  // instead of CU_LAUNCH_PARAM_BUFFER_POINTER (packed buffer).
+                  // The params array method is more robust for reduction operators with small
+                  // userdata structs (e.g., 1-byte REDOP structs), as it avoids alignment
+                  // issues that can cause CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES (error 701).
+                  // The last parameter (args + 1) points to the REDOP struct stored after KernelArgs.
+                  void *params[] = {&args->dst_base,   &args->dst_stride, &args->src_base,
+                                    &args->src_stride, &args->count,      args + 1};
+
                   CHECK_CU(CUDA_DRIVER_FNPTR(cuLaunchKernel)(
                       kernel, blocks_per_grid, 1, 1, threads_per_block, 1, 1,
-                      0 /*sharedmem*/, stream->get_stream(), 0 /*params*/, extra));
+                      0 /*sharedmem*/, stream->get_stream(), params, 0 /*extra*/));
                 } else {
+                  // Runtime API fallback path - also use params array for consistency
                   void *params[] = {&args->dst_base,   &args->dst_stride, &args->src_base,
                                     &args->src_stride, &args->count,      args + 1};
                   assert(redop->cudaLaunchKernel_fn != 0);
